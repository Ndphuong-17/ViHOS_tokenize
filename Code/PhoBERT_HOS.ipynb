{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd D:\\Project\\Toolkit_for_Preprocessing_MXH\\ViHOS_tokenize\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "I3YIeVxjOnfY"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "# clear gpu memory using torch\n",
        "torch.cuda.empty_cache()\n",
        "# clear output\n",
        "clear_output()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Y4KEsYU9Onff"
      },
      "outputs": [],
      "source": [
        "train_path = (r\"Data\\BIO_data\\train_BIO.csv\")\n",
        "dev_path = (r\"Data\\BIO_data\\dev_BIO.csv\")\n",
        "test_path = (r\"Data\\BIO_data\\test_BIO.csv\")\n",
        "test_index = 50 # default None value\n",
        "batch_size = 64\n",
        "max_len = 64\n",
        "shuffle = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Kxoa7w9yOnfg"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    XLMRobertaModel,\n",
        "    AutoTokenizer\n",
        ")\n",
        "\n",
        "input_model = XLMRobertaModel.from_pretrained(\"vinai/phobert-base\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
        "input_model.resize_token_embeddings(len(tokenizer))\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set: 363 samples\n",
            "Test set: 102 samples\n",
            "Development set: 125 samples\n"
          ]
        }
      ],
      "source": [
        "if test_index != None and test_index > 3:\n",
        "    # Load the data\n",
        "    train_path, dev_path, test_path = split_path(test_path, test_index, train_path, dev_path, test_path)\n",
        "elif test_index != None: \n",
        "    print(\"Test index out of range. Please provide a valid interger index greater than 3.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "w-Qyw31bOnfr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns: Index(['Unnamed: 0', 'index', 'Word', 'Tag', 'sentence_id'], dtype='object')\n",
            "Columns: Index(['Unnamed: 0', 'index', 'Word', 'Tag', 'sentence_id'], dtype='object')\n",
            "Columns: Index(['Unnamed: 0', 'index', 'Word', 'Tag', 'sentence_id'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "from Code.Dataset import split_path, create_dataloader\n",
        "\n",
        "train_dataloader = create_dataloader(train_path, batch_size=batch_size, tokenizer = tokenizer, max_len=max_len)\n",
        "dev_dataloader = create_dataloader(dev_path, batch_size=batch_size, tokenizer = tokenizer, max_len=max_len, shuffle=shuffle)\n",
        "test_dataloader = create_dataloader(test_path, batch_size=batch_size, tokenizer = tokenizer, max_len=max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.5727\n",
            "Validation Loss: 0.4313\n",
            "Span Macro F1-Score: 0.4986\n",
            "Epoch: 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.4113\n",
            "Validation Loss: 0.3138\n",
            "Span Macro F1-Score: 0.4987\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|██████████| 2/2 [00:08<00:00,  4.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Span F1 Score: 0.4994\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from Code.Model import setup_model, MultiTaskModel, train, test\n",
        "\n",
        "# Set up the model and training components\n",
        "model, criterion_span, optimizer_spans, device, num_epochs = setup_model(\n",
        "    input_model=input_model,\n",
        "    model_class=MultiTaskModel,\n",
        "    lr=5e-6,\n",
        "    weight_decay=1e-5,\n",
        "    num_epochs=2\n",
        ")\n",
        "\n",
        "# Now you can call your train and test functions with the returned objects\n",
        "train(\n",
        "    model=model,\n",
        "    train_dataloader=train_dataloader,\n",
        "    dev_dataloader=dev_dataloader,\n",
        "    criterion_span=criterion_span,\n",
        "    optimizer_spans=optimizer_spans,\n",
        "    device=device,\n",
        "    num_epochs=num_epochs\n",
        ")\n",
        "\n",
        "# Testing the model after training\n",
        "span_preds, span_targets = test(\n",
        "    model=model,\n",
        "    test_dataloader=test_dataloader,\n",
        "    device=device\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to trained_model.pth\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "test_results = {\n",
        "    \"predictions\": span_preds.tolist(),\n",
        "    \"targets\": span_targets.tolist()\n",
        "}\n",
        "with open('result.json', 'w') as f:\n",
        "    json.dump(test_results, f, indent=4)\n",
        "    # print(f\"Test results saved to {args.output_json}\")\n",
        "\n",
        "    # Save the trained model\n",
        "    model_save_path =\"trained_model.pth\"\n",
        "    torch.save(model.state_dict(), model_save_path)\n",
        "    print(f\"Model saved to {model_save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"D:\\Project\\Toolkit_for_Preprocessing_MXH\\ViHOS_tokenize\\Code\\main.py\", line 7, in <module>\n",
            "    from Code.Dataset import split_path, create_dataloader\n",
            "ModuleNotFoundError: No module named 'Code'\n"
          ]
        }
      ],
      "source": [
        "!python main.py --train_path \"Data/BIO_data/train_BIO.csv\" \\\n",
        "                     --dev_path \"Data/BIO_data/dev_BIO.csv\" \\\n",
        "                     --test_path \"Data/BIO_data/test_BIO.csv\" \\\n",
        "                     --test_index 50 \\\n",
        "                     --batch_size 64 \\\n",
        "                     --max_len 64 \\\n",
        "                     --lr 5e-6 \\\n",
        "                     --num_epochs 2 \\\n",
        "                     --output_json \"test_results.json\" \\\n",
        "                     --output_dir \"output\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
