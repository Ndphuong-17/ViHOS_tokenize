{"cells":[{"cell_type":"code","execution_count":10,"metadata":{"id":"_KFIOHagGKbc"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from ast import literal_eval"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1641556649094,"user":{"displayName":"Nap Lan Dau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyLJ7xblBnvLEyFishN4Q4xZSVXTxeaLzYRJU=s64","userId":"01595996840706054717"},"user_tz":-420},"id":"BSYQ85ybG7Hg","outputId":"e65ce208-1ccf-41ef-8e0b-d8b2a588b3c2"},"outputs":[],"source":["from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","\n","pd.set_option('display.max_rows', None)\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.width', None)\n","pd.set_option('display.max_colwidth', None)\n","pd.options.display.max_rows"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["D:\\Project\\Toolkit_for_Preprocessing_MXH\\ViHOS\n"]}],"source":["cd D:\\Project\\Toolkit_for_Preprocessing_MXH\\ViHOS"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["train_path = r\"Data\\Raw_data\\train.csv\"\n","dev_path = r\"Data\\Raw_data\\dev.csv\"\n","test_path = r\"Data\\Raw_data\\test.csv\""]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":191},"executionInfo":{"elapsed":341,"status":"ok","timestamp":1641570592726,"user":{"displayName":"Nap Lan Dau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyLJ7xblBnvLEyFishN4Q4xZSVXTxeaLzYRJU=s64","userId":"01595996840706054717"},"user_tz":-420},"id":"fn-nOSPGHMGw","outputId":"e51b26de-f1e2-43c7-dce2-05f35f416294"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>content</th>\n","      <th>index_spans</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>Anh bar .</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>Hello thầy</td>\n","      <td>[]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0     content index_spans\n","0           0   Anh bar .          []\n","1           1  Hello thầy          []"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["train = pd.read_csv(train_path)\n","dev = pd.read_csv(dev_path)\n","test = pd.read_csv(test_path)\n","test['index_spans'] = test['index_spans'].apply(literal_eval)\n","train['index_spans'] = train['index_spans'].apply(literal_eval)\n","dev['index_spans'] = dev['index_spans'].apply(literal_eval)\n","\n","headers = ['Unnamed: 0',  'content', 'index_spans']\n","train.columns = headers\n","dev.columns = headers\n","test.columns = headers\n","test.head(2)"]},{"cell_type":"markdown","metadata":{"id":"7t9vZ6idK66i"},"source":["# Pre-processing"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["from Code.Preprocessing import unicode\n","# Apply the replacement function to the 'content' column\n","test['content'] = test['content'].apply(unicode)\n","train['content'] = train['content'].apply(unicode)\n","dev['content'] = dev['content'].apply(unicode)\n"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["import os\n","folder_path = \"spans_text\"\n","if not os.path.exists(folder_path):\n","    # Create the folder if it doesn't exist\n","    os.makedirs(folder_path)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["df = train[['index_spans', 'content']]\n","df.columns = ['spans', 'text']\n","df.to_csv(r'spans_text\\df_train.csv')\n","\n","df = dev[['index_spans', 'content']]\n","df.columns = ['spans', 'text']\n","df.to_csv(r'spans_text\\df_dev.csv')\n","\n","df = test[['index_spans', 'content']]\n","df.columns = ['spans', 'text']\n","df.to_csv(r'spans_text\\df_test.csv')"]},{"cell_type":"markdown","metadata":{"id":"VvaXLDO6PIcA"},"source":["# Tokeniner"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"pnL6rnlKHsik"},"outputs":[],"source":["# from vncorenlp import VnCoreNLP\n","# annotator = VnCoreNLP(r\"D:\\Project\\Toolkit_for_Preprocessing_MXH\\ViHOS\\Code\\VnCoreNLP\", annotators=\"wseg\", max_heap_size='-Xmx500m')\n","\n","from vncorenlp import VnCoreNLP\n","\n","# Ensure that the JAR file path is correct\n","annotator = VnCoreNLP(r\"D:\\\\Project\\\\Toolkit_for_Preprocessing_MXH\\\\ViHOS\\\\Code\\\\VnCoreNLP\\\\VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')\n"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1641180328015,"user":{"displayName":"Lưu Đức Cảnh","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15467637707317635561"},"user_tz":-420},"id":"DtPB0PVULquy","outputId":"899752fe-72fb-41b1-9474-9a71e689d496"},"outputs":[{"data":{"text/plain":["['A', 'méo', 'nên', 'về', 'nước', 'thì', 'mới', 'đúng']"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["text = test['content'][325]\n","annotator_text = annotator.tokenize(text)\n","tokens = []\n","for i in range(len(annotator_text)):\n","  for j in range(len(annotator_text[i])):\n","    tokens.append(annotator_text[i][j])\n","tokens\n"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["# import more_itertools as mit\n","from itertools import groupby\n","from operator import itemgetter\n","\n","def find_ranges(span):\n","    # Group consecutive numbers and create ranges.\n","    # ex: [0, 1, 3, 20, 21] --> [(0, 1), (3,3), (20, 21)]\n","    return [(group[0], group[-1]) for _, g in groupby(enumerate(span), lambda x: x[0] - x[1])\n","            for group in [list(map(itemgetter(1), g))]]\n","\n","def tokenize_word(text, pos):\n","    tokens = [token for sentence in annotator.tokenize(text) for token in sentence]\n","    alignment, start = [], 0\n","\n","    for t in tokens:\n","        if t == \"_\":\n","            res = text.find(t, start)\n","        else:\n","            t = t.lstrip(\"_\").replace(\"_\", \" \")\n","            res = text.find(t, start)\n","\n","        alignment.append(pos[res:res + len(t)])\n","        start = res + len(t)\n","\n","    assert len(tokens) == len(alignment)\n","    return tokens, alignment\n","\n","def annotate(spans, alignment, tokens):\n","    annotations = pd.DataFrame({'Tokens': tokens, 'Tag': ['O'] * len(tokens)})\n","\n","    for span in spans:\n","        for i, align in enumerate(alignment):\n","            if align[-1] < span[0]:\n","                continue\n","            elif align[0] <= span[0] <= align[-1]:\n","                annotations.at[i, 'Tag'] = 'B-T'\n","            elif span[0] < align[0] <= span[-1]:\n","                annotations.at[i, 'Tag'] = 'I-T'\n","            elif align[0] > span[-1]:\n","                break\n","\n","    return annotations['Tag']\n","\n","def load_data(path):\n","    tsd = pd.read_csv(path)\n","    tsd['spans'] = tsd['spans'].apply(literal_eval)\n","\n","    data = []\n","    for _, row in tsd.iterrows():\n","        text, span = row['text'], row['spans']\n","        segments = find_ranges(span) if span else []\n","        temp = [[seg[0], seg[-1]] if len(seg) > 1 else [seg[0]] for seg in segments]\n","        text_spans = [text[seg[0]:seg[-1] + 1] for seg in segments]\n","        \n","        data.append({'text': text, 'spans': temp, 'text_spans': text_spans})\n","    \n","    return data\n","\n"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"Fia3UwtPpNxc"},"outputs":[{"data":{"text/plain":["{'text': 'đ m, thầy giáo cũng sống ảo',\n"," 'spans': [[0, 2]],\n"," 'text_spans': ['đ m']}"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["test_data = load_data(r'spans_text\\df_test.csv')\n","train_data = load_data(r'spans_text\\df_train.csv')\n","dev_data = load_data(r'spans_text\\df_dev.csv')\n","test_data[30]"]},{"cell_type":"markdown","metadata":{"id":"GraLvJq-sodS"},"source":["# Annotate BIO"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["def annotate(spans, alignment, tokens):\n","    # Initialize the DataFrame with tokens and default \"O\" tags\n","    annotations = pd.DataFrame({'Tokens': tokens, 'Tag': ['O'] * len(tokens)})\n","\n","    # Check if alignment is empty before processing spans\n","    if not alignment:\n","        return annotations['Tag']  # Return default \"O\" tags if alignment is empty\n","\n","    for span in spans:\n","        i = 0\n","        while i < len(alignment):\n","            # Check if alignment[i] has the expected structure\n","            if len(alignment[i]) < 2:  # Ensuring there are at least two elements\n","                i += 1\n","                continue\n","\n","            align_start, align_end = alignment[i][0], alignment[i][-1]\n","\n","            # Check if the current alignment ends before the span starts\n","            if align_end < span[0]:\n","                i += 1\n","            # Check for the beginning of the span\n","            elif align_start <= span[0] <= align_end:\n","                annotations.at[i, 'Tag'] = 'B-T'  # Beginning of the tag\n","                i += 1\n","            # Check for continuation of the span\n","            elif span[0] < align_start <= span[-1]:\n","                annotations.at[i, 'Tag'] = 'I-T'  # Inside of the tag\n","                i += 1\n","            # Stop if the current alignment starts after the span ends\n","            elif align_start > span[-1]:\n","                break\n","\n","    return annotations['Tag']"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from Code.Preprocessing import dupplicate_punctuation\n","\n","\n","def data_BIO(data):\n","    # Prepare a list to hold formatted data\n","    formated_data = []\n","\n","    for d in data:\n","        text = d['text']\n","        pos = list(range(len(text)))  # Create a position list\n","        text, pos = dupplicate_punctuation(text, pos)  # Clean up punctuation\n","        tokens, alignment = tokenize_word(text, pos)  # Tokenize the cleaned text\n","        annotations = annotate(d['spans'], alignment, tokens)  # Annotate the tokens\n","\n","        # Combine tokens and their corresponding annotations\n","        formated_data.extend(zip(tokens, annotations))  # Using zip for better performance\n","        formated_data.append((None, None))  # Append a marker for sentence separation\n","\n","\n","    # Create a DataFrame from the formatted data\n","    df_final = pd.DataFrame(formated_data, columns=['Word', 'Tag'])\n","\n","    # Generate sentence IDs\n","    sentence_id = []\n","    sentence = 0\n","    for word in df_final['Word']:\n","        if word is not None:\n","            sentence_id.append(sentence)\n","        else:\n","            sentence_id.append(np.nan)\n","            sentence += 1\n","\n","    df_final['sentence_id'] = sentence_id\n","    df_final.dropna(inplace=True)  # Remove rows where Word is None\n","    df_final['sentence_id'] = df_final['sentence_id'].astype(\"int64\")  # Convert to int64\n","\n","    return df_final\n"]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":21204,"status":"ok","timestamp":1641570640444,"user":{"displayName":"Nap Lan Dau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyLJ7xblBnvLEyFishN4Q4xZSVXTxeaLzYRJU=s64","userId":"01595996840706054717"},"user_tz":-420},"id":"aHoIEsFIrAt_","outputId":"42c7b9f6-57ce-4868-8b65-362a17d88da5"},"outputs":[],"source":["test_IBO = data_BIO(test_data)\n","train_IBO = data_BIO(train_data)\n","dev_IBO = data_BIO(dev_data)\n","test_IBO.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_IBO[30]"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"_DJC2DoD6z9Y"},"outputs":[],"source":["test_IBO.reset_index(inplace=True)\n","dev_IBO.reset_index(inplace=True)\n","train_IBO.reset_index(inplace=True)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["import os\n","folder_path = \"BIO_data\"\n","if not os.path.exists(folder_path):\n","    # Create the folder if it doesn't exist\n","    os.makedirs(folder_path)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"sYxSQkjOvPGe"},"outputs":[{"ename":"NameError","evalue":"name 'train_IBO' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_IBO\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBIO_data\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtrain_BIO.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m dev_IBO\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBIO_data\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdev_BIO.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m test_IBO\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBIO_data\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtest_BIO.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","\u001b[1;31mNameError\u001b[0m: name 'train_IBO' is not defined"]}],"source":["train_IBO.to_csv(r'BIO_data\\train_BIO.csv', index=False)\n","dev_IBO.to_csv(r'BIO_data\\dev_BIO.csv', index=False)\n","test_IBO.to_csv(r'BIO_data\\test_BIO.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":0}
